import os
import time
from urllib import request
import sqlite3
import argparse
import pprint
import random
from operator import itemgetter
from tqdm import tqdm

pp = pprint.PrettyPrinter(indent=4)

pwd = os.getcwd()

# Arguments
parser = argparse.ArgumentParser(description="Downloader for e6 based on tag for machine learning models")
parser.add_argument("--out", help="Where the downloaded images and tags reside. Separate from the DB.", required=True)
parser.add_argument("--e6", help="Location of the e6 DB generated by e6_export_to_sqlite.py", required=False, default=pwd + "/db/e6.db")
parser.add_argument("--db", help="Location of the DB to download images.", required=False, default=pwd + "/db/regenerated_downloaded.db")
parser.add_argument("--images", default=False, action="store_true", help="Will download images.")
parser.add_argument("--overwrite_images", default=False, action="store_true", help="Overwrites images if they already exist.")
parser.add_argument("--tags", default=False, action="store_true", help="Will only process tags.")
parser.add_argument("--overwrite_tags", default=False, action="store_true", help="Overwrites tags if they already exist.")
parser.add_argument("--test_tags", default=False, action="store_true", help="Tests a post ID.")
parser.add_argument("--verify_integrity", default=False, action="store_true", help="Scans the directory and checks if file tags match the post in the e6 DB.")
args = parser.parse_args()

# Auto Complete CSV data here
all_tags = {}

if args.out == "" or args.out == None:
	Exception("No directory supplied, stopping.")

# Strip any directory names ending with a slash
if args.out[len(args.out)-1] == "/" or args.out[len(args.out)-1] == "\\":
	args.out = args.out[:-1]

if not os.path.isdir(args.out):
	os.mkdir(args.out)

# Basic tag pruning lists
if not os.path.isfile(pwd + "/tag_meta_whitelist.conf"):
	with open(pwd + "/tag_meta_whitelist.conf") as file:
		file.write("")

meta_whitelist = []
with open(pwd + "/tag_meta_whitelist.conf", "r", encoding="utf-8") as meta_file:
	tags = meta_file.readlines()
	for tag in tags:
		new_tag = tag.strip().lower()
		if new_tag not in meta_whitelist:
			meta_whitelist.append(new_tag)

if not os.path.isfile(pwd + "/tag_species_blacklist.conf"):
	with open(pwd + "/tag_species_blacklist.conf") as file:
		file.write("")

species_blacklist = {}
with open(pwd + "/tag_species_blacklist.conf", "r", encoding="utf-8") as species_file:
	tags = species_file.readlines()
	for tag in tags:
		new_tag = tag.strip().lower()
		if new_tag not in species_blacklist:
			species_blacklist[new_tag] = True

# Get undesired tags from the bot's list.
undesired_tags = {}
if os.path.isfile(os.getcwd() + "/undesired_tags.conf"):
	with open("undesired_tags.conf", "r") as ut:
		lines = ut.readlines()
		for line in lines:
			undesired_tag = line.strip()
			# Deduplicate and ensure that the string has content
			if undesired_tag not in undesired_tags and undesired_tag != "":
				if undesired_tag.startswith("#"):
					continue
				else:
					undesired_tags[undesired_tag] = True

# Database stuff
def load_db():
	__con = sqlite3.connect(args.db)
	__cur = __con.cursor()
	return __con, __cur

con, cur = load_db()

def load_e6():
	memcon = sqlite3.connect(":memory:")
	__con = sqlite3.connect(args.e6)
	__con.backup(memcon)
	__cur = memcon.cursor()
	return memcon, __cur

econ, ecur = load_e6()

bad_post_ids = []
if os.path.isfile(os.getcwd() + "/db/failed_posts.txt"):
	with open(os.getcwd() + "/db/failed_posts.txt", "r", encoding="utf-8") as file:
		lines = file.readlines()
		for line in lines:
			id = line.strip()
			if id != "":
				bad_post_ids.append(int(id))

def get_e6_post(con, cur, id):
	query = f"SELECT * FROM posts WHERE post = {id}"
	for row in cur.execute(query):
		return row

def get_tag_alias(cur, _tag):
	# Scan for an existing alias
	query = f'SELECT COUNT(1) FROM alias WHERE old=?'
	for row in cur.execute(query, (_tag,)):
		if not row[0]:
			return _tag

	# If there is an alias, use that
	query = f'SELECT * FROM alias WHERE old=?'
	for row in cur.execute(query, (_tag,)):
		return row[1]

def filter_tags(cur, _tag):
	query = f'SELECT * FROM tags WHERE tag=? LIMIT 1'
	for row in cur.execute(query, (_tag,)):
		# Drop tags from these categories
		if int(row[1]) in [-1, 3, 6, 8]:
			return True
		# Also drop tags if they fail the whitelist/blacklist for meta and species
		# Drop meta tags that are not in the whitelist
		elif _tag not in meta_whitelist and int(row[1]) == 7:
			return True
		# Drop species tags that are in the blacklist
		elif _tag in species_blacklist and int(row[1]) == 5:
			return True
		
	return False

def process_tags_v2(con, cur, tags):
	# tags tend to be split by spaces when read from the posts table, so let's make a list
	unprocessed_tags = tags.split(" ")
	processed_tags = []
	tag_string = ""
	for itag in unprocessed_tags:
		tag = get_tag_alias(cur, itag)
		processed_tags.append(tag)
		query = f'SELECT * FROM implicate_lut WHERE tag=? LIMIT 1'
		for row in cur.execute(query, (tag,)):
			processed_tags = processed_tags + row[1].split(" ")
			break
		
	final_tags = []
	for tag in processed_tags:
		drop_tag = filter_tags(cur, tag)
		# Do not drop rating tags
		if tag == "safe":
			drop_tag = False
		elif tag == "questionable":
			drop_tag = False
		elif tag == "explicit":
			drop_tag = False
		# Do not process any further
		if not drop_tag:
			final_tags.append(tag)

	# Deduplicate
	final_tags = list(dict.fromkeys(final_tags))
	if args.test_tags:
		final_tags.sort(key=str.lower)
	else:
		random.shuffle(final_tags)
	
	pos = 1
	for tag in final_tags:
		ctag = tag.replace("_", " ")
		ctag = ctag.replace('"', "")
		ctag = ctag.replace("'", "")
		add_to_string = True
		if tag in all_tags:
			all_tags[tag][0] += 1
		elif tag not in all_tags:
			if tag == "safe":
				all_tags[tag] = [1, 0, ctag]
			elif tag == "questionable":
				all_tags[tag] = [1, 0, ctag]
			elif tag == "explicit":
				all_tags[tag] = [1, 0, ctag]
			else:
				# If the tag exists in the DB, add it to the CSV
				query = f'SELECT COUNT(1) FROM tags WHERE tag=?'
				for row in cur.execute(query, (tag,)):
					if not row[0]:
						add_to_string = False
				
				if add_to_string:
					query = f'SELECT * FROM tags WHERE tag=?'
					for row in cur.execute(query, (tag,)):
						# Store the count, category and post processed name
						all_tags[tag] = [1, row[1], ctag]
						break
		
		if add_to_string:
			if pos == len(final_tags):
				tag_string += ctag
			else:
				tag_string += f"{ctag}, "
		pos += 1
	return tag_string
		
def mkdir(hash1, hash2):
	b1 = f"{args.out}/{hash1}"
	b2 = f"{args.out}/{hash1}/{hash2}"
	if not os.path.isdir(b1):
		os.mkdir(b1)
	if not os.path.isdir(b2):
		os.mkdir(b2)

def download_e6_post(_con, _cur, _id):
	base_url = "https://static1.e621.net/data/"
	end_url = "?download=1"
	post = get_e6_post(_con, _cur, _id)

	# Download image data
	if post[6] == "f":
		img_filename = f"{_id}.{post[4]}"
		hash_f1 = post[1][0:2]
		hash_f2 = post[1][2:4]
		mkdir(hash_f1, hash_f2)
		out = f"{args.out}/{hash_f1}/{hash_f2}/{img_filename}"
		if not os.path.isfile(out) or args.overwrite_images:
			url = f"{base_url}{hash_f1}/{hash_f2}/{post[1]}.{post[4]}{end_url}"
			try:
				request.urlretrieve(url, out)
			except:
				bad_post_ids.append(_id)

def list_e6_tags(_con, _cur, _id):
	post = get_e6_post(_con, _cur, _id)
	tags = post[3]
	if post[2] == "e":
		tags += f" explicit"
	elif post[2] == "q":
		tags += f" questionable"
	elif post[2] == "s":
		tags += f" safe"
	return process_tags_v2(_con, _cur, tags)

def get_e6_tags(_con, _cur, _id):
	post = get_e6_post(_con, _cur, _id)
	tags = post[3]
	tag_filename = f"{_id}.txt"
	if post[2] == "e":
		tags += f" explicit"
	elif post[2] == "q":
		tags += f" questionable"
	elif post[2] == "s":
		tags += f" safe"
	clean_tags = process_tags_v2(_con, _cur, tags)
	hash_f1 = post[1][0:2]
	hash_f2 = post[1][2:4]
	mkdir(hash_f1, hash_f2)
	out = f"{args.out}/{hash_f1}/{hash_f2}/{tag_filename}"
	if not os.path.isfile(out) or args.overwrite_tags:
		with open(out, "w", encoding="utf-8") as file:
			file.write(clean_tags)

cur.execute("SELECT * FROM posts")
res = cur.fetchall()
lres = len(res)
del res

def progress_bar(current, total, bar_length=20):
	fraction = current / total

	arrow = int(fraction * bar_length) * "â–ˆ"
	padding = int(bar_length - len(arrow)) * " "

	ending = '\n' if current == total else '\r'

	return f'{int(fraction*100)}%|{arrow}{padding}|', ending

def get_time(seconds):
	if seconds >= 60*60:
		return str(time.strftime("%H:%M:%S", time.gmtime(seconds)))
	else:
		return str(time.strftime("%M:%S", time.gmtime(seconds)))

def get_list_avg(list):
	return sum(list) / len(list)

numimg = 0
dl_query = "SELECT * FROM posts"
init_time = time.perf_counter()
avg_times = []
console_len = ""
if args.images:
	print("Image Downloading:")
	for row in cur.execute(dl_query):
		loop_start = time.perf_counter()
		if row[0] not in bad_post_ids:
			download_e6_post(econ, ecur, row[0])
		loop_end = time.perf_counter()
		old_len = len(console_len)
		tot_time = loop_end - init_time
		loop_time = loop_end - loop_start
		avg_times.append(loop_time)
		true_avg = get_list_avg(avg_times)
		bar, end = progress_bar(numimg, lres, bar_length=30)
		console_len = f" {bar} D:{numimg}/{lres} [{get_time(tot_time)}<{get_time((true_avg)*(lres-numimg))}, {loop_time:.2f}s/img, {true_avg:.2f}s/avg]"
		new_len = len(console_len)
		if old_len > new_len:
			console_len = console_len + (old_len - new_len) * " "
			
		if numimg != lres:
			end = "\r"
		else:
			end = "\n"
		print(console_len, end=end)
		numimg += 1
	
	bad_posts = ""
	for post in bad_post_ids:
		bad_posts += f"{post}\n"

	with open(pwd + "/db/failed_posts.txt", "w", encoding="utf-8") as bad_file:
		bad_file.write(bad_posts)

numimg = 0
avg_times = []
if args.tags:
	print("Tags Processing:")
	for row in cur.execute(dl_query):
		loop_start = time.perf_counter()
		#if row[0] not in bad_post_ids:
		get_e6_tags(econ, ecur, row[0])
		loop_end = time.perf_counter()
		old_len = len(console_len)
		tot_time = loop_end - init_time
		loop_time = loop_end - loop_start
		avg_times.append(loop_time)
		true_avg = get_list_avg(avg_times)
		bar, end = progress_bar(numimg, lres, bar_length=30)
		console_len = f" {bar} D:{numimg}/{lres} [{get_time(tot_time)}<{get_time((true_avg)*(lres-numimg))}, {loop_time:.2f}s/post, {true_avg:.2f}s/avg]"
		new_len = len(console_len)
		if old_len > new_len:
			console_len = console_len + (old_len - new_len) * " "
			
		if numimg != lres:
			end = "\r"
		else:
			end = "\n"
		print(console_len, end=end)
		numimg += 1

	all_tag_tup = []
	for tag in all_tags.keys():
		# localised name 0, category 1, count 2, raw name 3
		all_tag_tup.append((all_tags[tag][2], all_tags[tag][1], all_tags[tag][0], tag))
	all_tag_tup = sorted(all_tag_tup, key=itemgetter(2), reverse=True)

	csv_data = ""
	csv_data_undesired = ""
	for tag_pair in all_tag_tup:
		out = f'{tag_pair[0]},{tag_pair[1]},{tag_pair[2]}\n'
		if tag_pair[3] not in undesired_tags and tag_pair[3] != "total_images":
			csv_data_undesired += out
		csv_data += out

	with open(pwd + "/db/model_tags_true.csv", "w", encoding="utf-8") as true_file:
		true_file.write(csv_data)

	with open(pwd + "/db/model_tags.csv", "w", encoding="utf-8") as csv_file:
		csv_file.write(csv_data_undesired)

if args.verify_integrity and not args.tags:
	print("Verifying Tag Integrity:")
	# Check each file properly
	for filename in tqdm(os.listdir(args.out)):
		file_tags = []
		file = os.path.splitext(filename)[0].lower()
		ext = os.path.splitext(filename)[1].lower()
		if ext in ['.txt']:
			current = os.path.join(args.out, filename)
			if os.path.isfile(current):
				# Read in tags from file
				with open(current, "r", encoding="utf-8") as tagfile:
					lines = tagfile.readlines()
					for line in lines:
						tags = line.strip().lower()
						one_line_tag = tags.split(", ")
						file_tags = list(dict.fromkeys(one_line_tag))
				
				# Grab tags from the e6 DB
				e6_tags = list_e6_tags(econ, ecur, file)
				e_list = dict.fromkeys(e6_tags)
				file_tags.sort(key=str.lower)
				
				# Scan through tags to find bad offenders
				for tag in file_tags:
					# Overwrite on a bad file
					if tag not in e_list:
						bad_post_ids.append(str(file))
						with open(f"{args.out}/{file}", "w", encoding="utf-8") as tag_file:
							tag_file.write(e6_tags)
						break
	print(f"{len(bad_post_ids)} files repaired.")

	with open(pwd + "/db/repaired_posts.txt", "w", encoding="utf-8") as bad_file:
		bad_file.writelines(bad_post_ids)
	
	# CSV generation
	all_tag_tup = []
	for tag in all_tags.keys():
		# localised name 0, category 1, count 2, raw name 3
		all_tag_tup.append((all_tags[tag][2], all_tags[tag][1], all_tags[tag][0], tag))
	all_tag_tup = sorted(all_tag_tup, key=itemgetter(2), reverse=True)

	csv_data = ""
	csv_data_undesired = ""
	for tag_pair in all_tag_tup:
		out = f'{tag_pair[0]},{tag_pair[1]},{tag_pair[2]}\n'
		if tag_pair[3] not in undesired_tags and tag_pair[3] != "total_images":
			csv_data_undesired += out
		csv_data += out

	with open(pwd + "/db/model_tags_true.csv", "w", encoding="utf-8") as true_file:
		true_file.write(csv_data)

	with open(pwd + "/db/model_tags.csv", "w", encoding="utf-8") as csv_file:
		csv_file.write(csv_data_undesired)

if args.test_tags:
	print(list_e6_tags(econ, ecur, 2278196))