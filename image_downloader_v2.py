import os
import time
from urllib import request
import sqlite3
import argparse
import pprint
import random
from operator import itemgetter

pp = pprint.PrettyPrinter(indent=4)

pwd = os.getcwd()

# Arguments
parser = argparse.ArgumentParser(description="Downloader for e6 based on tag for machine learning models")
parser.add_argument("--out", help="Where the downloaded images and tags reside. Separate from the DB.", required=True)
parser.add_argument("--e6", help="Location of the e6 DB generated by e6_export_to_sqlite.py", required=False, default=pwd + "/db/e6.db")
parser.add_argument("--db", help="Location of the DB to download images.", required=False, default=pwd + "/db/regenerated_downloaded.db")
args = parser.parse_args()

# Auto Complete CSV data here
all_tags = {}

if args.out == "" or args.out == None:
	Exception("No directory supplied, stopping.")

# Strip any directory names ending with a slash
if args.out[len(args.out)-1] == "/" or args.out[len(args.out)-1] == "\\":
	args.out = args.out[:-1]

if not os.path.isdir(args.out):
	os.mkdir(args.out)

# Basic tag pruning lists
if not os.path.isfile(pwd + "/tag_meta_whitelist.conf"):
	with open(pwd + "/tag_meta_whitelist.conf") as file:
		file.write("")

meta_whitelist = []
with open(pwd + "/tag_meta_whitelist.conf", "r", encoding="utf-8") as meta_file:
	tags = meta_file.readlines()
	for tag in tags:
		new_tag = tag.strip().lower()
		if new_tag not in meta_whitelist:
			meta_whitelist.append(new_tag)

if not os.path.isfile(pwd + "/tag_species_blacklist.conf"):
	with open(pwd + "/tag_species_blacklist.conf") as file:
		file.write("")

species_blacklist = []
with open(pwd + "/tag_species_blacklist.conf", "r", encoding="utf-8") as species_file:
	tags = species_file.readlines()
	for tag in tags:
		new_tag = tag.strip().lower()
		if new_tag not in species_blacklist:
			species_blacklist.append(new_tag)

# Get undesired tags from the bot's list.
undesired_tags = []
if os.path.isfile(os.getcwd() + "/undesired_tags.conf"):
	with open("undesired_tags.conf", "r") as ut:
		lines = ut.readlines()
		for line in lines:
			undesired_tag = line.strip()
			# Deduplicate and ensure that the string has content
			if undesired_tag not in undesired_tags and undesired_tag != "":
				if undesired_tag.startswith("#"):
					continue
				else:
					undesired_tags.append(undesired_tag)

# Database stuff
def load_db():
	__con = sqlite3.connect(args.db)
	__cur = __con.cursor()
	return __con, __cur

con, cur = load_db()

def load_e6():
	memcon = sqlite3.connect(":memory:")
	__con = sqlite3.connect(args.e6)
	__con.backup(memcon)
	__cur = memcon.cursor()
	return memcon, __cur

econ, ecur = load_e6()

bad_post_ids = []

def get_e6_post(con, cur, id):
	query = f"SELECT COUNT(1) FROM posts WHERE post = {id}"
	for row in cur.execute(query):
		if not row[0]:
			return (False, (0, "nopost", "s", "invalid", "png", 0))

	query = f"SELECT * FROM posts WHERE post = {id}"
	for row in cur.execute(query):
		return (True, row)

def process_tags(con, cur, tags):
	# tags tend to be split by spaces when read from the posts table, so let's make a list
	unprocessed_tags = tags.split(" ")

	# Alias tags first
	aliased_tags = []
	for tag in unprocessed_tags:
		atag = tag
		# Scan for an existing alias
		has_alias = True
		query = f'SELECT COUNT(1) FROM alias WHERE old=?'
		for row in cur.execute(query, (atag,)):
			if not row[0]:
				has_alias = False
				break

		# If there is an alias, use that
		if has_alias:
			query = f'SELECT * FROM alias WHERE old=?'
			for row in cur.execute(query, (atag,)):
				atag = row[1]
				break
		
		# Add to the list
		aliased_tags.append(atag)

	# Implicated tags, ie where having fox adds the canid tag. This may introduce duplicates
	implicated_tags = aliased_tags
	for tag in aliased_tags:
		itag = tag
		
		# Append any and all tags
		query = f'SELECT * FROM implicate WHERE tag=?'
		for row in cur.execute(query, (itag,)):
			implicated_tags.append(row[1])

	# Drop tags that are in the following categories, 
	# copyright 3, invalid -1 / 6, lore 8,
	# we keep general 0, artist 1, character 4, species 5 and meta 7
	dropped_tags = []
	for tag in implicated_tags:
		dtag = tag
		drop_tag = False
		query = f'SELECT * FROM tags WHERE tag =?'
		for row in cur.execute(query, (dtag,)):
			if int(row[1]) in [-1, 3, 6, 8]:
				drop_tag = True
			# Also drop tags if they fail the whitelist/blacklist for meta and species
			if tag not in meta_whitelist and row[1] == 7:
				drop_tag = True
			if tag in species_blacklist and row[1] == 5:
				drop_tag = True

		if not drop_tag:
			dropped_tags.append(dtag)
	
	# Deduplicate
	dedup_tags = list(dict.fromkeys(dropped_tags))

	final_tags = []
	# Clean up tags and add known tags to the autocomplete CSV:
	for tag in dedup_tags:
		ctag = tag.replace("_", " ")
		if tag in all_tags:
			all_tags[tag][0] += 1
		elif tag not in all_tags:
			query = f'SELECT * FROM tags WHERE tag =?'
			for row in cur.execute(query, (tag,)):
				# Store the count, category and post processed name
				all_tags[tag] = [1, row[1], ctag]

		final_tags.append(ctag)
	
	# Randomly shuffle tags
	#random.shuffle(final_tags)
	final_tags.sort(key=str.lower)

	# Finally, return a processed string
	tag_string = ""
	pos = 1
	for tag in final_tags:
		if pos == len(final_tags):
			tag_string += tag
		else:
			tag_string += f"{tag}, "
		pos += 1
	return tag_string

def get_tag_alias(cur, _tag):
	# Scan for an existing alias
	query = f'SELECT COUNT(1) FROM alias WHERE old=?'
	for row in cur.execute(query, (_tag,)):
		if not row[0]:
			return _tag

	# If there is an alias, use that
	query = f'SELECT * FROM alias WHERE old=?'
	for row in cur.execute(query, (_tag,)):
		return row[1]

def get_tag_implicates(cur, _tag):
	implicates = []

	# Append any and all tags
	query = f'SELECT * FROM implicate WHERE tag=?'
	for row in cur.execute(query, (_tag,)):
		implicates.append(row[1])
	return implicates

def filter_tags(cur, _tag):
	drop_tag = False
	query = f'SELECT * FROM tags WHERE tag =?'
	for row in cur.execute(query, (_tag,)):
		if int(row[1]) in [-1, 3, 6, 8]:
			drop_tag = True
		# Also drop tags if they fail the whitelist/blacklist for meta and species
		if tag not in meta_whitelist and row[1] == 7:
			drop_tag = True
		if tag in species_blacklist and row[1] == 5:
			drop_tag = True
		
	return drop_tag


def process_tags_v2(con, cur, tags):
	# tags tend to be split by spaces when read from the posts table, so let's make a list
	unprocessed_tags = tags.split(" ")
	processed_tags = []
	implicates = []
	tag_string = ""
	for itag in unprocessed_tags:
		tag = get_tag_alias(cur, itag)
		processed_tags.append(tag)
	
	implicates = processed_tags
	for itag in processed_tags:
		# Append any and all tags
		query = f'SELECT * FROM implicate WHERE tag=?'
		for row in cur.execute(query, (itag,)):
			implicates.append(row[1])
		
	final_tags = []
	for tag in implicates:
		drop_tag = filter_tags(cur, tag)
		# Do not process any further
		if not drop_tag:
			final_tags.append(tag)

	# Deduplicate
	final_tags = list(dict.fromkeys(final_tags))
	random.shuffle(final_tags)
	#final_tags.sort(key=str.lower)

	pos = 1
	for tag in final_tags:
		ctag = tag.replace("_", " ")
		if tag in all_tags:
			all_tags[tag][0] += 1
		elif tag not in all_tags:
			query = f'SELECT * FROM tags WHERE tag =?'
			for row in cur.execute(query, (tag,)):
				# Store the count, category and post processed name
				all_tags[tag] = [1, row[1], ctag]
				break

		if pos == len(final_tags):
			tag_string += ctag
		else:
			tag_string += f"{ctag}, "
		pos += 1
	return tag_string
		
def download_e6_post(_con, _cur, _id):
	base_url = "https://static1.e621.net/data/"
	end_url = "?download=1"
	post = get_e6_post(_con, _cur, _id)

	# Download image data
	if post[0]:
		img_filename = f"{_id}.{post[1][4]}"
		if not os.path.isfile(args.out + "/" + img_filename):
			url = f"{base_url}{post[1][1][0:2]}/{post[1][1][2:4]}/{post[1][1]}.{post[1][4]}{end_url}"
			try:
				request.urlretrieve(url, args.out + "/" + img_filename)
			except:
				bad_post_ids.append(_id)

def get_e6_tags(_con, _cur, _id):
	post = get_e6_post(_con, _cur, _id)
	if post[0]:
		tags = post[1][3]
		tag_filename = f"{_id}.txt"
		if post[1][2] == "e":
			tags += f" explicit"
		elif post[1][2] == "q":
			tags += f" questionable"
		elif post[1][2] == "s":
			tags += f" safe"
		clean_tags = process_tags_v2(_con, _cur, tags)
		if not os.path.isfile(args.out + "/" + tag_filename):
			with open(f"{args.out}/{tag_filename}", "w", encoding="utf-8") as file:
				file.write(clean_tags)

# test_id = 2470517
# download_e6_post(econ, ecur, test_id)
cur.execute("SELECT * FROM posts")
res = cur.fetchall()
lres = len(res)
del res

def progress_bar(current, total, bar_length=20):
	fraction = current / total

	arrow = int(fraction * bar_length) * "â–ˆ"
	padding = int(bar_length - len(arrow)) * " "

	ending = '\n' if current == total else '\r'

	return f'{int(fraction*100)}%|{arrow}{padding}|', ending

def get_time(seconds):
	if seconds >= 60*60:
		return str(time.strftime("%H:%M:%S", time.gmtime(seconds)))
	else:
		return str(time.strftime("%M:%S", time.gmtime(seconds)))

def get_list_avg(list):
	return sum(list) / len(list)

numimg = 0
dl_query = "SELECT * FROM posts"
init_time = time.perf_counter()
avg_times = []
console_len = ""
print("First Pass, Image Downloading:")
for row in cur.execute(dl_query):
	loop_start = time.perf_counter()
	download_e6_post(econ, ecur, row[0])
	loop_end = time.perf_counter()
	old_len = len(console_len)
	tot_time = loop_end - init_time
	loop_time = loop_end - loop_start
	avg_times.append(loop_time)
	true_avg = get_list_avg(avg_times)
	bar, end = progress_bar(numimg, lres, bar_length=30)
	console_len = f" {bar} D:{numimg}/{lres} [{get_time(tot_time)}<{get_time((true_avg)*(lres-numimg))}, {loop_time:.2f}s/img]"
	new_len = len(console_len)
	if old_len > new_len:
		console_len = console_len + (old_len - new_len) * " "
		
	if numimg != lres:
		end = "\r"
	else:
		end = "\n"
	print(console_len, end=end)
	numimg += 1

numimg = 0
avg_times = []
print("Second Pass, Tags Processing:")
for row in cur.execute(dl_query):
	loop_start = time.perf_counter()
	if row[0] not in bad_post_ids:
		get_e6_tags(econ, ecur, row[0])
	loop_end = time.perf_counter()
	old_len = len(console_len)
	tot_time = loop_end - init_time
	loop_time = loop_end - loop_start
	avg_times.append(loop_time)
	true_avg = get_list_avg(avg_times)
	bar, end = progress_bar(numimg, lres, bar_length=30)
	console_len = f" {bar} D:{numimg}/{lres} [{get_time(tot_time)}<{get_time((true_avg)*(lres-numimg))}, {loop_time:.2f}s/post]"
	new_len = len(console_len)
	if old_len > new_len:
		console_len = console_len + (old_len - new_len) * " "
		
	if numimg != lres:
		end = "\r"
	else:
		end = "\n"
	print(console_len, end=end)
	numimg += 1

all_tag_tup = []
for tag in all_tags.keys():
	# localised name 0, category 1, count 2, raw name 3
	all_tag_tup.append((all_tags[tag][2], all_tags[tag][1], all_tags[tag][0], tag))
all_tag_tup = sorted(all_tag_tup, key=itemgetter(2), reverse=True)

csv_data = ""
csv_data_undesired = ""
for tag_pair in all_tag_tup:
	out = f'{tag_pair[0]},{tag_pair[1]},{tag_pair[2]}\n'
	if tag_pair[3] not in undesired_tags and tag_pair[3] != "total_images":
		csv_data_undesired += out
	csv_data += out

with open(pwd + "/db/tags_true.csv", "w", encoding="utf-8") as true_file:
	true_file.write(csv_data)

with open(pwd + "/db/tags.csv", "w", encoding="utf-8") as csv_file:
	csv_file.write(csv_data_undesired)

bad_posts = ""
for post in bad_post_ids:
	bad_posts += f"{post}\n"

with open(pwd + "/db/failed_posts.txt", "w", encoding="utf-8") as bad_file:
	bad_file.write(bad_posts)